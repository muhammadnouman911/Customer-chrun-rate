{
 "cells": [
  {
   "cell_type": "code",
   "id": "aa017b489876731",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:15.560896Z",
     "start_time": "2024-08-01T07:07:15.557643Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "b6c28f786e008217",
   "metadata": {},
   "source": [
    "## Part-01 Data Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:17.542269Z",
     "start_time": "2024-08-01T07:07:17.388780Z"
    }
   },
   "source": [
    "df=pd.read_csv('Churn_Modelling.csv')\n",
    "print(df.head(10))\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "print(df.columns)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
      "0          1    15634602  Hargrave          619    France  Female   42   \n",
      "1          2    15647311      Hill          608     Spain  Female   41   \n",
      "2          3    15619304      Onio          502    France  Female   42   \n",
      "3          4    15701354      Boni          699    France  Female   39   \n",
      "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
      "5          6    15574012       Chu          645     Spain    Male   44   \n",
      "6          7    15592531  Bartlett          822    France    Male   50   \n",
      "7          8    15656148    Obinna          376   Germany  Female   29   \n",
      "8          9    15792365        He          501    France    Male   44   \n",
      "9         10    15592389        H?          684    France    Male   27   \n",
      "\n",
      "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
      "0       2       0.00              1          1               1   \n",
      "1       1   83807.86              1          0               1   \n",
      "2       8  159660.80              3          1               0   \n",
      "3       1       0.00              2          0               0   \n",
      "4       2  125510.82              1          1               1   \n",
      "5       8  113755.78              2          1               0   \n",
      "6       7       0.00              2          1               1   \n",
      "7       4  115046.74              4          1               0   \n",
      "8       4  142051.07              2          0               1   \n",
      "9       2  134603.88              1          1               1   \n",
      "\n",
      "   EstimatedSalary  Exited  \n",
      "0        101348.88       1  \n",
      "1        112542.58       0  \n",
      "2        113931.57       1  \n",
      "3         93826.63       0  \n",
      "4         79084.10       0  \n",
      "5        149756.71       1  \n",
      "6         10062.80       0  \n",
      "7        119346.88       1  \n",
      "8         74940.50       0  \n",
      "9         71725.73       0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   RowNumber        10000 non-null  int64  \n",
      " 1   CustomerId       10000 non-null  int64  \n",
      " 2   Surname          10000 non-null  object \n",
      " 3   CreditScore      10000 non-null  int64  \n",
      " 4   Geography        10000 non-null  object \n",
      " 5   Gender           10000 non-null  object \n",
      " 6   Age              10000 non-null  int64  \n",
      " 7   Tenure           10000 non-null  int64  \n",
      " 8   Balance          10000 non-null  float64\n",
      " 9   NumOfProducts    10000 non-null  int64  \n",
      " 10  HasCrCard        10000 non-null  int64  \n",
      " 11  IsActiveMember   10000 non-null  int64  \n",
      " 12  EstimatedSalary  10000 non-null  float64\n",
      " 13  Exited           10000 non-null  int64  \n",
      "dtypes: float64(2), int64(9), object(3)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "         RowNumber    CustomerId   CreditScore           Age        Tenure  \\\n",
      "count  10000.00000  1.000000e+04  10000.000000  10000.000000  10000.000000   \n",
      "mean    5000.50000  1.569094e+07    650.528800     38.921800      5.012800   \n",
      "std     2886.89568  7.193619e+04     96.653299     10.487806      2.892174   \n",
      "min        1.00000  1.556570e+07    350.000000     18.000000      0.000000   \n",
      "25%     2500.75000  1.562853e+07    584.000000     32.000000      3.000000   \n",
      "50%     5000.50000  1.569074e+07    652.000000     37.000000      5.000000   \n",
      "75%     7500.25000  1.575323e+07    718.000000     44.000000      7.000000   \n",
      "max    10000.00000  1.581569e+07    850.000000     92.000000     10.000000   \n",
      "\n",
      "             Balance  NumOfProducts    HasCrCard  IsActiveMember  \\\n",
      "count   10000.000000   10000.000000  10000.00000    10000.000000   \n",
      "mean    76485.889288       1.530200      0.70550        0.515100   \n",
      "std     62397.405202       0.581654      0.45584        0.499797   \n",
      "min         0.000000       1.000000      0.00000        0.000000   \n",
      "25%         0.000000       1.000000      0.00000        0.000000   \n",
      "50%     97198.540000       1.000000      1.00000        1.000000   \n",
      "75%    127644.240000       2.000000      1.00000        1.000000   \n",
      "max    250898.090000       4.000000      1.00000        1.000000   \n",
      "\n",
      "       EstimatedSalary        Exited  \n",
      "count     10000.000000  10000.000000  \n",
      "mean     100090.239881      0.203700  \n",
      "std       57510.492818      0.402769  \n",
      "min          11.580000      0.000000  \n",
      "25%       51002.110000      0.000000  \n",
      "50%      100193.915000      0.000000  \n",
      "75%      149388.247500      0.000000  \n",
      "max      199992.480000      1.000000  \n",
      "Index(['RowNumber', 'CustomerId', 'Surname', 'CreditScore', 'Geography',\n",
      "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
      "       'IsActiveMember', 'EstimatedSalary', 'Exited'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "e2b149e988e12a5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:18.957834Z",
     "start_time": "2024-08-01T07:07:18.947653Z"
    }
   },
   "source": [
    "X=df.iloc[:,3:-1].values\n",
    "Y=df.iloc[:,-1].values\n"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "940db5f38b02a85b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:20.709570Z",
     "start_time": "2024-08-01T07:07:20.453471Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder_x_1 = LabelEncoder()\n",
    "X[:, 1] = label_encoder_x_1.fit_transform(X[:, 1])\n",
    "label_encoder_x_2 = LabelEncoder()\n",
    "X[:, 2] = label_encoder_x_2.fit_transform(X[:, 2])\n",
    "X"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 0, 0, ..., 1, 1, 101348.88],\n",
       "       [608, 2, 0, ..., 0, 1, 112542.58],\n",
       "       [502, 0, 0, ..., 1, 0, 113931.57],\n",
       "       ...,\n",
       "       [709, 0, 0, ..., 0, 1, 42085.58],\n",
       "       [772, 1, 1, ..., 1, 0, 92888.52],\n",
       "       [792, 0, 0, ..., 1, 0, 38190.78]], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "c2dcf5871aa5a383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:22.742828Z",
     "start_time": "2024-08-01T07:07:22.673335Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "#Argument should be categorial data feild\n",
    "\n",
    "\n",
    "# Country column\n",
    "# Assuming X is your dataset\n",
    "# Define the ColumnTransformer with OneHotEncoder for the first column (index 0)\n",
    "# Assuming X is your dataset and it's a NumPy array or a DataFrame\n",
    "# Define the ColumnTransformer with OneHotEncoder for the first column (index 0)\n",
    "ct = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('encode', OneHotEncoder(drop='first'), [1])\n",
    "    ], remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Fit and transform the dataset\n",
    "X = np.array(ct.fit_transform(X)).astype('float')\n",
    "X\n",
    "# X=pd.DataFrame(X)\n",
    "# X.iloc[]\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 6.1900000e+02, ..., 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0134888e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 6.0800000e+02, ..., 0.0000000e+00,\n",
       "        1.0000000e+00, 1.1254258e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 5.0200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 1.1393157e+05],\n",
       "       ...,\n",
       "       [0.0000000e+00, 0.0000000e+00, 7.0900000e+02, ..., 0.0000000e+00,\n",
       "        1.0000000e+00, 4.2085580e+04],\n",
       "       [1.0000000e+00, 0.0000000e+00, 7.7200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 9.2888520e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 7.9200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 3.8190780e+04]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "beb2726361630e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:24.133254Z",
     "start_time": "2024-08-01T07:07:24.052640Z"
    }
   },
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "e9ef2da5c1bc7c83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:25.079751Z",
     "start_time": "2024-08-01T07:07:25.072818Z"
    }
   },
   "source": [
    "# We Need Feature Scaling\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "81472c6a4b1e9765",
   "metadata": {},
   "source": [
    "## Part 02 - ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67922b7be5fe84a",
   "metadata": {},
   "source": [
    "### 1. import keras\n",
    "This imports the Keras library. Keras provides an easy-to-use interface for creating and training deep learning models. It's known for its simplicity and user-friendly API.\n",
    "\n",
    "### 2. from keras.models import Sequential\n",
    "Sequential: This is a type of neural network model in Keras. It's a linear stack of layers, where you add one layer at a time in a sequence. It's one of the simplest and most commonly used models for building neural networks. Each layer has weights that are updated during training.\n",
    "### 3. from keras.layers import Dense\n",
    "Dense: This is a type of layer used in neural networks, particularly in fully connected (dense) layers. Each neuron in a Dense layer receives input from every neuron in the previous layer, which makes it \"dense\" in terms of connections. It's commonly used in feedforward neural networks and is a building block for many types of models."
   ]
  },
  {
   "cell_type": "code",
   "id": "d53a521fed45c1fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:31.974944Z",
     "start_time": "2024-08-01T07:07:27.209328Z"
    }
   },
   "source": [
    "import keras \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 12:07:28.642247: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "4af13d733b6b175f",
   "metadata": {},
   "source": [
    "### Initilaizing the ANN\n",
    "\n",
    "The code `classifier = Sequential()` initializes a neural network using the Sequential API from Keras, a popular deep learning library. Here's a simple explanation of each part:\n",
    "\n",
    "1. **`classifier`**: This is the name you've given to your neural network model. You can name it anything you like.\n",
    "   \n",
    "2. **`Sequential()`**: This is a class from Keras that allows you to build a neural network layer by layer in a sequential manner. This means you can add layers to your model one after the other, from the input layer to the output layer.\n",
    "\n",
    "The purpose of this code is to create an empty neural network model that you can later add layers to. By using the `Sequential` class, you're telling Keras that your neural network will have a simple, linear stack of layers, where each layer has exactly one input tensor and one output tensor. This is the most common way to build neural networks in Keras.\n",
    "\n",
    "So, this line of code is just the first step in constructing your neural network. Next, you'll add layers to this `classifier` model to define its structure and functionality."
   ]
  },
  {
   "cell_type": "code",
   "id": "8c44e942c9657078",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:32.064725Z",
     "start_time": "2024-08-01T07:07:31.978069Z"
    }
   },
   "source": [
    "classifier= Sequential() # Initialized Neural Network\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-01 12:07:32.001894: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "138f3bfa5839fdd4",
   "metadata": {},
   "source": [
    "#### Each Feature is going to be input node\n",
    "#### adding the input layer and first hidden layer\n",
    "#### We will use Sigmoid for the output layer and Rectifier Function for the Hidden Layer\n",
    "\n",
    "\n",
    "#### Adding the Input Layer and First Hidden Layer\n",
    "\n",
    "```python\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **`classifier.add(...)`**:\n",
    "   - This method adds a new layer to the neural network model named `classifier`.\n",
    "\n",
    "2. **`Dense(...)`**:\n",
    "   - A `Dense` layer is a fully connected layer where each neuron is connected to every neuron in the previous layer.\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **`units=6`**:\n",
    "  - Specifies the number of neurons in this hidden layer. Here, the hidden layer has 6 neurons. The number of neurons is often chosen based on experimentation or heuristics, like averaging the number of neurons in the input and output layers.\n",
    "\n",
    "- **`kernel_initializer='uniform'`**:\n",
    "  - Defines the method for initializing the weights. 'Uniform' initializes weights with small random values drawn from a uniform distribution. This helps in breaking symmetry and starting the training process.\n",
    "\n",
    "- **`activation='relu'`**:\n",
    "  - Specifies the activation function applied to the neurons in this layer. 'ReLU' (Rectified Linear Unit) outputs the input directly if it is positive, and zero otherwise. This introduces non-linearity, which is crucial for learning complex patterns.\n",
    "\n",
    "- **`input_dim=11`**:\n",
    "  - Defines the number of input features (or the number of neurons in the input layer). This parameter is required only for the first layer to specify the shape of the input data. Subsequent layers infer their input dimensions from the previous layer.\n",
    "\n",
    "### What This Code Does:\n",
    "\n",
    "- **Adds the First Hidden Layer**: Constructs the first hidden layer with 6 neurons. This layer is connected to the input layer (implicitly created) and processes the input data using the ReLU activation function.\n",
    "- **Initializes Weights**: The weights of the neurons are initialized with small random values within a uniform range. This helps the model start learning.\n",
    "- **Applies Activation Function**: The ReLU activation function is used to introduce non-linearity, which helps the network learn and model complex patterns.\n",
    "- **Defines Input Shape**: By setting `input_dim=11`, the code specifies that the input layer will handle data with 11 features.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Input Layer**: Implicitly created by specifying `input_dim=11` in the first hidden layer. This defines the shape of the input data.\n",
    "- **First Hidden Layer**: Adds a hidden layer with 6 neurons, initializes weights uniformly, applies the ReLU activation function, and processes input data.\n",
    "\n",
    "This documentation provides a detailed explanation of how the input layer and the first hidden layer are added to the neural network, ensuring clarity on each component's role and function.\n",
    "\n",
    "### Remember\n",
    "\n",
    "Input Layer: The input layer is implicitly created when you define the first hidden layer and specify the input_dim parameter. This tells Keras how many input features to expect.\n",
    "\n",
    "First Hidden Layer: The first hidden layer is then added on top of this implicit input layer. In your code, this hidden layer has 6 neurons and uses the ReLU activation function."
   ]
  },
  {
   "cell_type": "code",
   "id": "c78a8e11948dcdf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:46.391591Z",
     "start_time": "2024-08-01T07:07:46.332694Z"
    }
   },
   "source": [
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=11))\n",
    "\n",
    "# output_dim : no of node you want to add in the hidden layer - Optimal No : avg(input_layer * output_layer)\n",
    "# init: randomly initialize the weights with 'uniform'\n",
    "# activation ( function in hidden layer ) : Rectifier for hidden layer - relu\n",
    "# input_dim : No of Node in the input layer which is no of variables but it will not be required for the next hidden layers"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "da551facd7ad82b7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Adding Second Hidden Layer\n",
    "\n",
    "Adding a second hidden layer to your neural network can enhance the model's ability to learn and represent more complex patterns in the data. Here's a detailed explanation of the code and general guidelines for working with hidden layers:\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "```python\n",
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))\n",
    "```\n",
    "\n",
    "- **`classifier.add(...)`**: Adds a new layer to the neural network model named `classifier`.\n",
    "\n",
    "- **`Dense(...)`**: A fully connected layer where each neuron is connected to every neuron in the previous layer.\n",
    "\n",
    "- **`units=6`**: Specifies the number of neurons in this second hidden layer. This layer has 6 neurons, similar to the first hidden layer.\n",
    "\n",
    "- **`kernel_initializer='uniform'`**: Specifies the method for initializing the weights. 'Uniform' initializes weights with small random values drawn from a uniform distribution.\n",
    "\n",
    "- **`activation='relu'`**: Specifies the activation function for the neurons in this layer. 'ReLU' (Rectified Linear Unit) is used, which introduces non-linearity by outputting the input directly if it is positive, or zero otherwise.\n",
    "\n",
    "### What is it Doing?\n",
    "\n",
    "- **Adding a Second Hidden Layer**: This line of code adds a second hidden layer with 6 neurons to the model. This layer processes the output of the first hidden layer, allowing the network to learn more complex representations of the data.\n",
    "\n",
    "### General Guidelines for Hidden Layers:\n",
    "\n",
    "1. **Why Add Hidden Layers?**\n",
    "   - Hidden layers enable the network to learn complex patterns and representations in the data. Each hidden layer captures different features and abstractions, contributing to the model's ability to handle intricate tasks.\n",
    "\n",
    "2. **How Many Hidden Layers?**\n",
    "   - The number of hidden layers depends on the problem's complexity. Simple problems may require only one or two hidden layers, while complex problems (e.g., image recognition, natural language processing) often benefit from deeper networks with many hidden layers.\n",
    "\n",
    "3. **Number of Neurons in Each Layer:**\n",
    "   - The number of neurons in each hidden layer is determined based on the complexity of the problem and experimentation. Common practices include:\n",
    "     - Using the average of the number of neurons in the input and output layers.\n",
    "     - Experimenting with different configurations and using cross-validation to find the optimal setup.\n",
    "\n",
    "4. **Avoiding Overfitting:**\n",
    "   - Adding too many layers and neurons can lead to overfitting, where the model performs well on training data but poorly on unseen test data. Techniques to mitigate overfitting include dropout, regularization, and early stopping.\n",
    "\n",
    "### Example of a Two Hidden Layer Neural Network:\n",
    "\n",
    "Here's a summary of your current network structure:\n",
    "\n",
    "- **Input Layer** (implicitly defined): 11 neurons (based on `input_dim=11` in the first hidden layer).\n",
    "- **First Hidden Layer**: 6 neurons, ReLU activation.\n",
    "- **Second Hidden Layer**: 6 neurons, ReLU activation.\n",
    "\n",
    "This configuration allows your neural network to learn more complex patterns compared to a single hidden layer network. Experimentation with additional layers or different neuron counts can help you find the best architecture for your specific problem.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to adjust the number of neurons or layers based on your experimentation and specific requirements."
   ]
  },
  {
   "cell_type": "code",
   "id": "8a9332b23eb05548",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:50.161037Z",
     "start_time": "2024-08-01T07:07:50.107713Z"
    }
   },
   "source": [
    "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu'))"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "1b734b43557490db",
   "metadata": {},
   "source": [
    "Here's the updated documentation for adding the output layer to your neural network:\n",
    "\n",
    "---\n",
    "\n",
    "## Adding the Output Layer\n",
    "\n",
    "The output layer of a neural network is crucial as it determines how the network's predictions are represented. The setup for the output layer depends on whether you're dealing with a binary or multi-class classification problem.\n",
    "\n",
    "### Binary vs. Multi-Class Classification:\n",
    "\n",
    "- **Binary Classification**:\n",
    "  - For problems with two categories (e.g., yes/no, 0/1), you use the following configuration for the output layer:\n",
    "\n",
    "  ```python\n",
    "  classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "  ```\n",
    "\n",
    "- **Multi-Class Classification**:\n",
    "  - For problems with more than two categories, you need to adjust the `units` and `activation` parameters:\n",
    "\n",
    "  ```python\n",
    "  classifier.add(Dense(units=num_classes, kernel_initializer='uniform', activation='softmax'))\n",
    "  ```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Binary Classification Configuration**:\n",
    "   - **`units=1`**: This specifies a single neuron in the output layer because there are only two possible outcomes.\n",
    "   - **`kernel_initializer='uniform'`**: Initializes the weights with a uniform distribution, similar to previous layers.\n",
    "   - **`activation='sigmoid'`**: The 'sigmoid' activation function outputs a probability value between 0 and 1, suitable for binary classification.\n",
    "\n",
    "2. **Multi-Class Classification Configuration**:\n",
    "   - **`units=num_classes`**: Set this to the number of classes in your problem. For instance, if you have 3 categories, use `units=3`.\n",
    "   - **`kernel_initializer='uniform'`**: Initializes the weights with a uniform distribution.\n",
    "   - **`activation='softmax'`**: The 'softmax' activation function converts the output to a probability distribution over all classes, ensuring that the probabilities sum up to 1. This is suitable for multi-class classification problems where each input belongs to one of the multiple classes.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **Binary Classification Output Layer**:\n",
    "  - **`units=1`**: Single output neuron.\n",
    "  - **`activation='sigmoid'`**: Outputs a probability for binary classification.\n",
    "\n",
    "- **Multi-Class Classification Output Layer**:\n",
    "  - **`units=num_classes`**: Number of neurons equal to the number of classes.\n",
    "  - **`activation='softmax'`**: Outputs a probability distribution across multiple classes.\n",
    "\n",
    "This setup ensures that your neural network's output layer aligns with the type of classification problem you're working on, providing the appropriate format for making predictions.\n",
    "\n",
    "---\n",
    "\n",
    "Feel free to adjust the `units` and `activation` parameters according to your specific classification problem."
   ]
  },
  {
   "cell_type": "code",
   "id": "6abed0f21f579c23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:54.351260Z",
     "start_time": "2024-08-01T07:07:54.292347Z"
    }
   },
   "source": [
    "classifier.add(Dense(units=1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "# If you are dealing with dependent variable more than two categories,CHANGES : - output_dim= \"no of classes (i.e 3 categories for dependent variable)\" -activation='soft-max' i.e Sigmoid for more than two dependent categories\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "id": "7fa6964d27e258f1",
   "metadata": {},
   "source": [
    "# Compiling ANN i.e applying sophastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720b88d2f8bbd818",
   "metadata": {},
   "source": [
    "\n",
    "### Code for Compiling the Model:\n",
    "\n",
    "```python\n",
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **`classifier.compile(...)`**: This method configures the model for training. It specifies the optimizer, the loss function, and the metrics to monitor during training and evaluation.\n",
    "\n",
    "2. **`optimizer='adam'`**:\n",
    "   - **Optimizer**: An optimizer is an algorithm used to adjust the weights of the neural network in order to minimize the loss function. The `adam` optimizer stands for Adaptive Moment Estimation. It combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "   - **Why Adam?**: Adam is popular because it is computationally efficient, has low memory requirements, and is well-suited for problems with large data sets and high-dimensional parameter spaces.\n",
    "\n",
    "3. **`loss='binary_crossentropy'`**:\n",
    "   - **Loss Function**: The loss function is used to evaluate how well the model's predictions match the actual labels. During training, the model tries to minimize this loss.\n",
    "   - **`binary_crossentropy`**: This loss function is used for binary classification problems. It measures the performance of a classification model whose output is a probability value between 0 and 1. It is the same as the logarithmic loss used in logistic regression. For binary outcomes (e.g., yes/no, 0/1), `binary_crossentropy` is appropriate.\n",
    "\n",
    "   - **For Multi-Class Classification**:\n",
    "     - **`categorical_crossentropy`**: If you were dealing with more than two categories, you would use `categorical_crossentropy` as the loss function. This is because `categorical_crossentropy` is designed to handle multi-class classification problems, where the output is a probability distribution over multiple classes.\n",
    "\n",
    "4. **`metrics=['accuracy']`**:\n",
    "   - **Metrics**: Metrics are used to judge the performance of your model. During training and evaluation, these metrics are monitored.\n",
    "   - **`accuracy`**: Accuracy is the ratio of correctly predicted instances to the total instances. It is a common metric for classification problems. Using `accuracy` as a metric allows you to see how often the model's predictions match the actual labels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "640a5731363a706e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-01T07:07:58.097698Z",
     "start_time": "2024-08-01T07:07:58.044809Z"
    }
   },
   "source": [
    "classifier.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "f80684331c457191",
   "metadata": {},
   "source": [
    "## Fit the ANN to the Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f4b935051aa55",
   "metadata": {},
   "source": [
    "### Code for Training the Model:\n",
    "\n",
    "```python\n",
    "classifier.fit(X_train, Y_train, batch_size=10, nb_epoch=100)\n",
    "```\n",
    "\n",
    "### Explanation of the Code:\n",
    "\n",
    "1. **`classifier.fit(...)`**: This method trains the model on the given training data (`X_train` and `Y_train`).\n",
    "\n",
    "2. **`X_train`**: This is the input data used for training the model. It contains the features (independent variables).\n",
    "\n",
    "3. **`Y_train`**: This is the output data used for training the model. It contains the labels (dependent variables).\n",
    "\n",
    "4. **`batch_size=10`**:\n",
    "   - **Batch Size**: This specifies the number of training samples to use in one iteration before updating the model's weights. In this case, it is set to 10.\n",
    "   - **Why Batch Size?**: Training on the entire dataset at once can be computationally expensive and may not fit into memory. Using smaller batches allows the model to update its weights more frequently and can lead to faster convergence.\n",
    "\n",
    "5. **`nb_epoch=100`**:\n",
    "   - **Epoch**: An epoch refers to one complete pass through the entire training dataset. When you set `nb_epoch=100`, it means the model will go through the entire training dataset 100 times during the training process.\n",
    "   - **Why Multiple Epochs?**: Training for multiple epochs allows the model to learn better. The more times the model sees the data, the better it can learn and generalize.\n",
    "\n",
    "### Explanation of Epochs:\n",
    "\n",
    "- **Epoch**: An epoch is one complete iteration over the entire training dataset. During one epoch, the model processes every training sample exactly once. After each epoch, the model's weights are updated based on the cumulative error computed during that epoch.\n",
    "- **Multiple Epochs**: Training a model for multiple epochs means repeating this process multiple times. Each pass helps the model adjust its weights and biases more accurately, leading to better performance.\n",
    "\n",
    "### Example:\n",
    "\n",
    "If you have a training dataset with 1000 samples and you set `batch_size=10`, the model will process 10 samples at a time. This means:\n",
    "\n",
    "- **Iterations per Epoch**: Since the batch size is 10, and you have 1000 samples, each epoch will consist of 100 iterations (1000 samples / 10 samples per batch).\n",
    "- **Total Iterations**: With `nb_epoch=100`, the model will go through the dataset 100 times, resulting in 100 epochs. This means the model will perform a total of 10,000 iterations (100 epochs * 100 iterations per epoch).\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **`classifier.fit(X_train, Y_train, batch_size=10, nb_epoch=100)`**: This code trains the model using the training data (`X_train` and `Y_train`). The model updates its weights every 10 samples and repeats this process for 100 complete passes over the training dataset.\n",
    "- **Batch Size**: Determines how many samples the model processes before updating its weights.\n",
    "- **Epoch**: Refers to one complete pass through the entire training dataset. Training for multiple epochs helps the model learn better and generalize to unseen data.\n",
    "\n",
    "By adjusting the batch size and the number of epochs, you can control how the model learns and improves during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6e1026e131fd7fe1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T11:08:07.887929Z",
     "start_time": "2024-07-26T10:58:37.768684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "800/800 [==============================] - 8s 8ms/step - loss: 0.5123 - accuracy: 0.7960\n",
      "Epoch 2/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.4565 - accuracy: 0.7960\n",
      "Epoch 3/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.4454 - accuracy: 0.7960\n",
      "Epoch 4/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.4388 - accuracy: 0.7960\n",
      "Epoch 5/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.4340 - accuracy: 0.7960\n",
      "Epoch 6/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.4286 - accuracy: 0.7960\n",
      "Epoch 7/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.4230 - accuracy: 0.7960\n",
      "Epoch 8/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.4193 - accuracy: 0.7999\n",
      "Epoch 9/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.4168 - accuracy: 0.8269\n",
      "Epoch 10/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.4144 - accuracy: 0.8270\n",
      "Epoch 11/100\n",
      "800/800 [==============================] - 4s 6ms/step - loss: 0.4128 - accuracy: 0.8311\n",
      "Epoch 12/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.4109 - accuracy: 0.8301\n",
      "Epoch 13/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.4097 - accuracy: 0.8310\n",
      "Epoch 14/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.4082 - accuracy: 0.8306\n",
      "Epoch 15/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.4083 - accuracy: 0.8316\n",
      "Epoch 16/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.4072 - accuracy: 0.8316\n",
      "Epoch 17/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.4061 - accuracy: 0.8311\n",
      "Epoch 18/100\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.4058 - accuracy: 0.8326\n",
      "Epoch 19/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.4052 - accuracy: 0.8322\n",
      "Epoch 20/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.4047 - accuracy: 0.8314\n",
      "Epoch 21/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.4044 - accuracy: 0.8328\n",
      "Epoch 22/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.4037 - accuracy: 0.8326\n",
      "Epoch 23/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.4031 - accuracy: 0.8335\n",
      "Epoch 24/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.4030 - accuracy: 0.8335\n",
      "Epoch 25/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.4024 - accuracy: 0.8331\n",
      "Epoch 26/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.4023 - accuracy: 0.8354\n",
      "Epoch 27/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.4013 - accuracy: 0.8338\n",
      "Epoch 28/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.4017 - accuracy: 0.8335\n",
      "Epoch 29/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.4009 - accuracy: 0.8353\n",
      "Epoch 30/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.4008 - accuracy: 0.8338\n",
      "Epoch 31/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.4004 - accuracy: 0.8359\n",
      "Epoch 32/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.4003 - accuracy: 0.8346\n",
      "Epoch 33/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3998 - accuracy: 0.8347\n",
      "Epoch 34/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3992 - accuracy: 0.8365\n",
      "Epoch 35/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.3991 - accuracy: 0.8336\n",
      "Epoch 36/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3987 - accuracy: 0.8361\n",
      "Epoch 37/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3988 - accuracy: 0.8370\n",
      "Epoch 38/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3985 - accuracy: 0.8351\n",
      "Epoch 39/100\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.3979 - accuracy: 0.8380\n",
      "Epoch 40/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3980 - accuracy: 0.8350\n",
      "Epoch 41/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.3976 - accuracy: 0.8381\n",
      "Epoch 42/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3975 - accuracy: 0.8379\n",
      "Epoch 43/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3973 - accuracy: 0.8375\n",
      "Epoch 44/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3972 - accuracy: 0.8386\n",
      "Epoch 45/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3969 - accuracy: 0.8389\n",
      "Epoch 46/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3968 - accuracy: 0.8378\n",
      "Epoch 47/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.3968 - accuracy: 0.8370\n",
      "Epoch 48/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3968 - accuracy: 0.8371\n",
      "Epoch 49/100\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3965 - accuracy: 0.8388\n",
      "Epoch 50/100\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3962 - accuracy: 0.8396\n",
      "Epoch 51/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3960 - accuracy: 0.8379\n",
      "Epoch 52/100\n",
      "800/800 [==============================] - 10s 13ms/step - loss: 0.3963 - accuracy: 0.8393\n",
      "Epoch 53/100\n",
      "800/800 [==============================] - 8s 9ms/step - loss: 0.3961 - accuracy: 0.8372\n",
      "Epoch 54/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3957 - accuracy: 0.8400\n",
      "Epoch 55/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3955 - accuracy: 0.8396\n",
      "Epoch 56/100\n",
      "800/800 [==============================] - 9s 11ms/step - loss: 0.3958 - accuracy: 0.8381\n",
      "Epoch 57/100\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3953 - accuracy: 0.8390\n",
      "Epoch 58/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3952 - accuracy: 0.8390\n",
      "Epoch 59/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.3949 - accuracy: 0.8391\n",
      "Epoch 60/100\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3952 - accuracy: 0.8384\n",
      "Epoch 61/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3951 - accuracy: 0.8388\n",
      "Epoch 62/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3947 - accuracy: 0.8397\n",
      "Epoch 63/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3947 - accuracy: 0.8390\n",
      "Epoch 64/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3945 - accuracy: 0.8396\n",
      "Epoch 65/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3943 - accuracy: 0.8384\n",
      "Epoch 66/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3943 - accuracy: 0.8395\n",
      "Epoch 67/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3940 - accuracy: 0.8409\n",
      "Epoch 68/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3937 - accuracy: 0.8409\n",
      "Epoch 69/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3939 - accuracy: 0.8391\n",
      "Epoch 70/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3937 - accuracy: 0.8397\n",
      "Epoch 71/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3942 - accuracy: 0.8401\n",
      "Epoch 72/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3940 - accuracy: 0.8382\n",
      "Epoch 73/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.3940 - accuracy: 0.8394\n",
      "Epoch 74/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3939 - accuracy: 0.8393\n",
      "Epoch 75/100\n",
      "800/800 [==============================] - 4s 5ms/step - loss: 0.3936 - accuracy: 0.8385\n",
      "Epoch 76/100\n",
      "800/800 [==============================] - 7s 8ms/step - loss: 0.3935 - accuracy: 0.8395\n",
      "Epoch 77/100\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3935 - accuracy: 0.8403\n",
      "Epoch 78/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.3936 - accuracy: 0.8403\n",
      "Epoch 79/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3935 - accuracy: 0.8394\n",
      "Epoch 80/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3933 - accuracy: 0.8399\n",
      "Epoch 81/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.3932 - accuracy: 0.8413\n",
      "Epoch 82/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3933 - accuracy: 0.8388\n",
      "Epoch 83/100\n",
      "800/800 [==============================] - 3s 4ms/step - loss: 0.3933 - accuracy: 0.8388\n",
      "Epoch 84/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3930 - accuracy: 0.8400\n",
      "Epoch 85/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3929 - accuracy: 0.8390\n",
      "Epoch 86/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3933 - accuracy: 0.8401\n",
      "Epoch 87/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3933 - accuracy: 0.8390\n",
      "Epoch 88/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3926 - accuracy: 0.8395\n",
      "Epoch 89/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3928 - accuracy: 0.8397\n",
      "Epoch 90/100\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3923 - accuracy: 0.8382\n",
      "Epoch 91/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3927 - accuracy: 0.8403\n",
      "Epoch 92/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3925 - accuracy: 0.8399\n",
      "Epoch 93/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3928 - accuracy: 0.8395\n",
      "Epoch 94/100\n",
      "800/800 [==============================] - 5s 6ms/step - loss: 0.3923 - accuracy: 0.8390\n",
      "Epoch 95/100\n",
      "800/800 [==============================] - 6s 7ms/step - loss: 0.3925 - accuracy: 0.8404\n",
      "Epoch 96/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3925 - accuracy: 0.8395\n",
      "Epoch 97/100\n",
      "800/800 [==============================] - 5s 7ms/step - loss: 0.3923 - accuracy: 0.8403\n",
      "Epoch 98/100\n",
      "800/800 [==============================] - 6s 8ms/step - loss: 0.3922 - accuracy: 0.8400\n",
      "Epoch 99/100\n",
      "800/800 [==============================] - 7s 9ms/step - loss: 0.3924 - accuracy: 0.8372\n",
      "Epoch 100/100\n",
      "800/800 [==============================] - 8s 10ms/step - loss: 0.3924 - accuracy: 0.8409\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x73a779d47ed0>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train, Y_train, batch_size=10, epochs=100)\n",
    "\n",
    "\n",
    "# batch-size :  no opf observation you want to update weight - FIxed Choice : 10 \n",
    "#epoch: when the while training set pass to the ann - Fixed CHoice 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb4bfa7289b5ad",
   "metadata": {},
   "source": [
    "## More Explanation for Epoch\n",
    "\n",
    "Yes, you're correct! One complete iteration over the entire training dataset in an epoch involves both forward and backward propagation.\n",
    "\n",
    "### Forward and Backward Propagation:\n",
    "\n",
    "1. **Forward Propagation**:\n",
    "   - In forward propagation, the input data is passed through the network layer by layer to produce the output.\n",
    "   - During this process, each neuron calculates a weighted sum of its inputs and applies an activation function to generate its output.\n",
    "   - This continues until the output layer generates the final predictions for the given input data.\n",
    "\n",
    "2. **Backward Propagation (Backpropagation)**:\n",
    "   - After the forward pass, the model compares the predicted output with the actual output to compute the loss using the specified loss function (e.g., `binary_crossentropy`).\n",
    "   - Backpropagation involves calculating the gradient of the loss function with respect to each weight in the network, starting from the output layer and moving backward through the network.\n",
    "   - These gradients indicate how much a small change in each weight will affect the loss.\n",
    "   - The optimizer (e.g., Adam) then uses these gradients to update the weights in a direction that reduces the loss, improving the model's predictions.\n",
    "\n",
    "### Complete Iteration:\n",
    "\n",
    "- **Complete Iteration**: Refers to the process where the model performs both forward and backward propagation for a single batch of data.\n",
    "- **Epoch**: An epoch consists of many such complete iterations, covering the entire training dataset.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Given your training configuration:\n",
    "\n",
    "- **Batch Size**: 10\n",
    "- **Epochs**: 100\n",
    "- **Training Samples**: 1000\n",
    "\n",
    "For each epoch:\n",
    "1. The dataset is divided into 100 batches (1000 samples / 10 samples per batch).\n",
    "2. For each batch:\n",
    "   - Forward propagation is performed to compute the predictions.\n",
    "   - The loss is calculated by comparing the predictions with the actual labels.\n",
    "   - Backward propagation is performed to calculate the gradients.\n",
    "   - The weights are updated using the optimizer based on these gradients.\n",
    "3. This process is repeated for each of the 100 batches, completing one epoch.\n",
    "4. The entire process is repeated for 100 epochs, allowing the model to see the data multiple times and improve its performance with each pass.\n",
    "\n",
    "### Summary:\n",
    "\n",
    "- **One Complete Iteration**: Involves both forward and backward propagation for a single batch of data.\n",
    "- **Epoch**: Consists of many such complete iterations, ensuring that the model processes the entire training dataset once.\n",
    "\n",
    "By performing multiple epochs, the model can iteratively adjust its weights to minimize the loss and improve its accuracy on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031327cdddf6ff5",
   "metadata": {},
   "source": [
    "## Time to make prediction on Text Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ef6c1cb934b8d6b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T11:08:34.953569Z",
     "start_time": "2024-07-26T11:08:33.941912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred=classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e3daf68668dcd8ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T11:08:44.025499Z",
     "start_time": "2024-07-26T11:08:44.017610Z"
    }
   },
   "outputs": [],
   "source": [
    "Y_pred=(Y_pred >0.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13106f7e649b4",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4234f95d12d4eae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T11:08:51.498708Z",
     "start_time": "2024-07-26T11:08:50.453360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1547   48]\n",
      " [ 265  140]]\n",
      "0.47217537942664417\n",
      "0.7446808510638298\n",
      "0.345679012345679\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      1595\n",
      "           1       0.74      0.35      0.47       405\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.80      0.66      0.69      2000\n",
      "weighted avg       0.83      0.84      0.82      2000\n",
      "\n",
      "0.8435\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "If no scoring is specified, the estimator passed should have a 'score' method. The estimator <keras.engine.sequential.Sequential object at 0x73a77ad29650> does not.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[42], line 13\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(accuracy_score(Y_test, Y_pred))\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m cross_val_score\n\u001B[0;32m---> 13\u001B[0m \u001B[38;5;28mprint\u001B[39m(cross_val_score(classifier, X_train, Y_train, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m))\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(cross_val_score(classifier, X_train, Y_train, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124maccuracy\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28mprint\u001B[39m(cross_val_score(classifier, X_train, Y_train, cv\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, scoring\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/model_selection/_validation.py:717\u001B[0m, in \u001B[0;36mcross_val_score\u001B[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001B[0m\n\u001B[1;32m    582\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Evaluate a score by cross-validation.\u001B[39;00m\n\u001B[1;32m    583\u001B[0m \n\u001B[1;32m    584\u001B[0m \u001B[38;5;124;03mRead more in the :ref:`User Guide <cross_validation>`.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    714\u001B[0m \u001B[38;5;124;03m[0.3315057  0.08022103 0.03531816]\u001B[39;00m\n\u001B[1;32m    715\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    716\u001B[0m \u001B[38;5;66;03m# To ensure multimetric format is not supported\u001B[39;00m\n\u001B[0;32m--> 717\u001B[0m scorer \u001B[38;5;241m=\u001B[39m check_scoring(estimator, scoring\u001B[38;5;241m=\u001B[39mscoring)\n\u001B[1;32m    719\u001B[0m cv_results \u001B[38;5;241m=\u001B[39m cross_validate(\n\u001B[1;32m    720\u001B[0m     estimator\u001B[38;5;241m=\u001B[39mestimator,\n\u001B[1;32m    721\u001B[0m     X\u001B[38;5;241m=\u001B[39mX,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    731\u001B[0m     error_score\u001B[38;5;241m=\u001B[39merror_score,\n\u001B[1;32m    732\u001B[0m )\n\u001B[1;32m    733\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m cv_results[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_score\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_scorer.py:951\u001B[0m, in \u001B[0;36mcheck_scoring\u001B[0;34m(estimator, scoring, allow_none)\u001B[0m\n\u001B[1;32m    949\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    950\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 951\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    952\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf no scoring is specified, the estimator passed should \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    953\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhave a \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscore\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m method. The estimator \u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m does not.\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m estimator\n\u001B[1;32m    954\u001B[0m     )\n",
      "\u001B[0;31mTypeError\u001B[0m: If no scoring is specified, the estimator passed should have a 'score' method. The estimator <keras.engine.sequential.Sequential object at 0x73a77ad29650> does not."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score ,classification_report,accuracy_score\n",
    "cm=confusion_matrix(Y_test, Y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f9c230419a4f3c2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T11:10:35.972010Z",
     "start_time": "2024-07-26T11:10:35.939820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8435\n",
      "F1 Score: 0.47217537942664417\n",
      "Precision: 0.7446808510638298\n",
      "Recall: 0.345679012345679\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.97      0.91      1595\n",
      "           1       0.74      0.35      0.47       405\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.80      0.66      0.69      2000\n",
      "weighted avg       0.83      0.84      0.82      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score, classification_report, accuracy_score\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(Y_test, Y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Compute F1 score\n",
    "f1 = f1_score(Y_test, Y_pred)\n",
    "print(\"F1 Score:\", f1)\n",
    "\n",
    "# Compute precision\n",
    "precision = precision_score(Y_test, Y_pred)\n",
    "print(\"Precision:\", precision)\n",
    "\n",
    "# Compute recall\n",
    "recall = recall_score(Y_test, Y_pred)\n",
    "print(\"Recall:\", recall)\n",
    "\n",
    "# Compute classification report\n",
    "classification_rep = classification_report(Y_test, Y_pred)\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
